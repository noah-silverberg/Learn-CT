{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2faf75b",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression Tutorial\n",
    "\n",
    "This short notebook gives a self‑contained example of **Gaussian Process (GP) regression** on a toy 1‑D dataset.\n",
    "We implement the RBF kernel and prediction formulas *manually* so that every equation is transparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4162ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rbf_kernel(x1, x2, length_scale=1.0, sigma_f=1.0):\n",
    "    \"\"\"Isotropic squared‑exponential kernel.\"\"\"\n",
    "    sqdist = np.sum(x1**2,1).reshape(-1,1) + np.sum(x2**2,1) - 2*np.dot(x1, x2.T)\n",
    "    return sigma_f**2 * np.exp(-0.5 / length_scale**2 * sqdist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586bfd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic training data\n",
    "rng = np.random.default_rng(0)\n",
    "X_train = np.linspace(-5, 5, 25).reshape(-1,1)\n",
    "y_train = np.sin(X_train[:,0]) + 0.3*rng.standard_normal(X_train.shape[0])\n",
    "\n",
    "# Dense test grid\n",
    "X_test = np.linspace(-6, 6, 400).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48933611",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale = 1.0\n",
    "sigma_f = 1.0\n",
    "sigma_y = 0.3    # noise std\n",
    "\n",
    "# Kernel (training, training)\n",
    "K = rbf_kernel(X_train, X_train, length_scale, sigma_f) + sigma_y**2 * np.eye(len(X_train))\n",
    "\n",
    "# Kernel (training, test) and (test, test)\n",
    "K_s = rbf_kernel(X_train, X_test, length_scale, sigma_f)\n",
    "K_ss = rbf_kernel(X_test, X_test, length_scale, sigma_f)\n",
    "\n",
    "# Invert K\n",
    "K_inv = np.linalg.inv(K)\n",
    "\n",
    "# Predictive mean and covariance\n",
    "mu_s = K_s.T @ K_inv @ y_train\n",
    "cov_s = K_ss - K_s.T @ K_inv @ K_s\n",
    "std_s = np.sqrt(np.diag(cov_s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb0867",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.scatter(X_train[:,0], y_train, marker='x', label='train data')\n",
    "plt.plot(X_test[:,0], mu_s, label='predictive mean')\n",
    "plt.fill_between(X_test[:,0], mu_s-2*std_s, mu_s+2*std_s, alpha=0.2, label='95% conf.')\n",
    "plt.legend()\n",
    "plt.title('Gaussian Process Regression')\n",
    "plt.xlabel('x'); plt.ylabel('f(x)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a2daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw posterior samples\n",
    "rng = np.random.default_rng(1)\n",
    "L = np.linalg.cholesky(cov_s + 1e-6*np.eye(len(X_test)))\n",
    "f_post = mu_s + L @ rng.standard_normal((len(X_test), 3))\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "for i in range(3):\n",
    "    plt.plot(X_test[:,0], f_post[:,i])\n",
    "plt.scatter(X_train[:,0], y_train, marker='x', c='k')\n",
    "plt.title('Posterior samples')\n",
    "plt.xlabel('x'); plt.ylabel('f(x)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
